{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Reinforcement Learning in Gym's Lunar Lander Environment\n",
    "\n",
    "It took some experimentation, but the steps to record video output in LunarLander-v2 since the Monitor wrapper was removed using an anaconda environment is as follows: \n",
    "\n",
    "1. Install any packages with conda package manager, e.g. `conda install -c conda-forge moviepy`\n",
    "2. Create environment with argument `render_mode='rgb_array'`\n",
    "3. Use new wrapper `gym.wrappers.record_video.RecordVideo`\n",
    "4. Wrap like so `RecordVideo(env, \"Recordings\")`\n",
    "\n",
    "Let's initialize a LunarLander-v2 environmnet, make random actions in the environment then view a recording of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/alex/Documents/Programming/Jupyter Notebooks/Deep RL Lunar Lander/Recordings/random-movements-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/alex/Documents/Programming/Jupyter Notebooks/Deep RL Lunar Lander/Recordings/random-movements-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/alex/Documents/Programming/Jupyter Notebooks/Deep RL Lunar Lander/Recordings/random-movements-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from gym.wrappers.record_video import RecordVideo\n",
    "\n",
    "env = gym.make('LunarLander-v2', render_mode='rgb_array')\n",
    "env = RecordVideo(env, \"Recordings\", name_prefix=\"random-movements\")\n",
    "env.reset(seed=42)\n",
    "\n",
    "terminated, truncated = False, False\n",
    "while not terminated or truncated:\n",
    "    action = env.action_space.sample()  # Take a random action\n",
    "    _, _, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Information\n",
    "This information is from the official Gym documentation.\n",
    "\n",
    "https://www.gymlibrary.dev/environments/box2d/lunar_lander/\n",
    "\n",
    "| Feature Category  | Details                                |\n",
    "|-------------------|----------------------------------------|\n",
    "| Action Space      | Discrete(4)                            |\n",
    "| Observation Shape | (8,)                                   |\n",
    "| Observation High  | [1.5 1.5 5. 5. 3.14 5. 1. 1. ]         |\n",
    "| Observation Low   | [-1.5 -1.5 -5. -5. -3.14 -5. -0. -0. ] |\n",
    "| Import            | `gym.make(\"LunarLander-v2\")`           |\n",
    "\n",
    "## Description of Environment\n",
    "\n",
    "This environment is a classic rocket trajectory optimization problem. According to Pontryagin’s maximum principle, it is optimal to fire the engine at full throttle or turn it off. This is the reason why this environment has discrete actions: engine on or off.\n",
    "\n",
    "There are two environment versions: discrete or continuous. The landing pad is always at coordinates `(0,0)`. The coordinates are the first two numbers in the state vector. Landing outside of the landing pad is possible. Fuel is infinite, so an agent could learn to fly and then land on its first attempt.\n",
    "\n",
    "## Action Space\n",
    "There are four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine.\n",
    "\n",
    "| Action  | Result                          |\n",
    "|---------|---------------------------------|\n",
    "| 0       | Do nothing                      |\n",
    "| 1       | Fire left orientation engine    |\n",
    "| 2       | Fire main engine                |\n",
    "| 3       | Fire right orientation engine   |\n",
    "\n",
    "## Observation Space\n",
    "The state is an 8-dimensional vector: the coordinates of the lander in `x` & `y`, its linear velocities in `x` & `y`, its angle, its angular velocity, and two booleans that represent whether each leg is in contact with the ground or not.\n",
    "\n",
    "| Observation  | Value                                   |\n",
    "|--------------|-----------------------------------------|\n",
    "| 0            | `x` coordinate (float)                  |\n",
    "| 1            | `y` coordinate (float)                  |\n",
    "| 2            | `x` linear velocity (float)             |\n",
    "| 3            | `y` linear velocity (float)             |\n",
    "| 4            | Angle in radians from -π to +π (float)  |\n",
    "| 5            | Angular velocity (float)                |\n",
    "| 6            | Left leg contact (bool)                 |\n",
    "| 7            | Right leg contact (bool)                |\n",
    "\n",
    "## Rewards\n",
    "Reward for moving from the top of the screen to the landing pad and coming to rest is about 100-140 points. If the lander moves away from the landing pad, it loses reward. If the lander crashes, it receives an additional -100 points. If it comes to rest, it receives an additional +100 points. Each leg with ground contact is +10 points. Firing the main engine is -0.3 points each frame. Firing the side engine is -0.03 points each frame. Solved is 200 points.\n",
    "\n",
    "## Starting State\n",
    "The lander starts at the top center of the viewport with a random initial force applied to its center of mass.\n",
    "\n",
    "## Episode Termination\n",
    "The episode finishes if:\n",
    "\n",
    "1. The lander crashes (the lander body gets in contact with the moon);\n",
    "\n",
    "2. The lander gets outside of the viewport (`x` coordinate is greater than 1);\n",
    "\n",
    "3. The lander is not awake. From the Box2D docs, a body which is not awake is a body which doesn’t move and doesn’t collide with any other body:\n",
    "\n",
    "---\n",
    "\n",
    "## The Safe Agent\n",
    "We're going to implement a simple agent 'The Safe Agent' who will thrust upward if and only if the lander's `y` position is less than 0.5.\n",
    "\n",
    "In theory this agent shouldn't hit the ground as we have unlimited fuel, but let's see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/alex/Documents/Programming/Jupyter Notebooks/Deep RL Lunar Lander/Recordings/safe-agent-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/alex/Documents/Programming/Jupyter Notebooks/Deep RL Lunar Lander/Recordings/safe-agent-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/alex/Documents/Programming/Jupyter Notebooks/Deep RL Lunar Lander/Recordings/safe-agent-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "class SafeAgent:\n",
    "    def __init__(self):\n",
    "        self.total_reward = 0\n",
    "        self.n_steps = 0\n",
    "        self.done = False\n",
    "\n",
    "    def reset(self, env):\n",
    "        self.total_reward = 0\n",
    "        self.n_steps = 0\n",
    "        self.done = False\n",
    "\n",
    "        # New API format returns observations and info (which we don't need)\n",
    "        self.state, _ = env.reset(seed=42)\n",
    "\n",
    "    def act(self, state):\n",
    "        MIN_HEIGHT = 1\n",
    "\n",
    "        if state[1] < MIN_HEIGHT:\n",
    "            return 2\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def step(self, env):\n",
    "        action = self.act(self.state)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        self.total_reward += reward\n",
    "        self.state = next_obs\n",
    "        self.done = terminated or truncated\n",
    "        \n",
    "\n",
    "env = gym.make('LunarLander-v2', render_mode='rgb_array')\n",
    "env = RecordVideo(env, \"Recordings\", name_prefix=\"safe-agent\")\n",
    "agent = SafeAgent()\n",
    "\n",
    "def play_episode(env, agent):\n",
    "    agent.reset(env=env)\n",
    "\n",
    "    while not agent.done:\n",
    "        agent.step(env)\n",
    "\n",
    "play_episode(env, agent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Stable Agent\n",
    "Let's try to define and agent that can remain stable in the air.\n",
    "\n",
    "It will operate via the following rules:\n",
    "\n",
    "1. If below height of 1: action = 2 (main engine)\n",
    "2. If angle is above π/50: action = 1 (fire right engine)\n",
    "3. If angle is above π/50: action = 1 (fire left engine)\n",
    "4. If x distance is above 0.4: action = 3 (fire left engine)\n",
    "5. If x distance is below -0.4: action = 1 (fire left engine)\n",
    "6. If below height of 1.5: action = 2 (main engine)\n",
    "6. Else: action = 0 (do nothing)\n",
    "\n",
    "The idea is the lander will always use its main engine if it falls below a certain height, next it will prioritize stabilizing the angle of the lander, then the distance, then keeping it above another height. \n",
    "\n",
    "Let's see how this approach does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/alex/Documents/Programming/Jupyter Notebooks/Deep RL Lunar Lander/Recordings/stable-agent-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/alex/Documents/Programming/Jupyter Notebooks/Deep RL Lunar Lander/Recordings/stable-agent-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/alex/Documents/Programming/Jupyter Notebooks/Deep RL Lunar Lander/Recordings/stable-agent-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "class StableAgent:\n",
    "    def __init__(self):\n",
    "        self.total_reward = 0\n",
    "        self.n_steps = 0\n",
    "        self.done = False\n",
    "\n",
    "    def reset(self, env):\n",
    "        self.total_reward = 0\n",
    "        self.n_steps = 0\n",
    "        self.done = False\n",
    "\n",
    "        # New API format returns observations and info (which we don't need)\n",
    "        self.state, _ = env.reset(seed=42)\n",
    "\n",
    "    def act(self, state):\n",
    "        UPPER_MIN_Y = 1.5\n",
    "        LOWER_MIN_Y = 1\n",
    "        MIN_X = -0.4\n",
    "        MAX_X = 0.4\n",
    "        MIN_ANGLE = -3.14/50\n",
    "        MAX_ANGLE = 3.14/50\n",
    "\n",
    "        x = state[0]\n",
    "        y = state[1]\n",
    "\n",
    "        angle = state[4]\n",
    "\n",
    "        MAIN_ENGINE = 2\n",
    "        LEFT_ENGINE = 1\n",
    "        RIGHT_ENGINE = 3\n",
    "        DO_NOTHING = 0\n",
    "\n",
    "        if y < LOWER_MIN_Y:\n",
    "            return MAIN_ENGINE\n",
    "\n",
    "        elif angle > MAX_ANGLE:\n",
    "            return RIGHT_ENGINE\n",
    "        elif angle < MIN_ANGLE:\n",
    "            return LEFT_ENGINE\n",
    "        \n",
    "        elif x > MAX_X:\n",
    "            return LEFT_ENGINE\n",
    "        elif x < MIN_X:\n",
    "            return RIGHT_ENGINE\n",
    "        \n",
    "        elif y < UPPER_MIN_Y:\n",
    "            return MAIN_ENGINE\n",
    "        \n",
    "        else:\n",
    "            return DO_NOTHING\n",
    "        \n",
    "\n",
    "        \n",
    "    def step(self, env):\n",
    "        action = self.act(self.state)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        \n",
    "        self.total_reward += reward\n",
    "        self.state = next_obs\n",
    "        self.done = terminated or truncated\n",
    "        \n",
    "\n",
    "env = gym.make('LunarLander-v2', render_mode='rgb_array')\n",
    "env = RecordVideo(env, \"Recordings\", name_prefix=\"stable-agent\")\n",
    "agent = StableAgent()\n",
    "\n",
    "def play_episode(env, agent):\n",
    "    agent.reset(env=env)\n",
    "\n",
    "    while not agent.done:\n",
    "        agent.step(env)\n",
    "\n",
    "play_episode(env, agent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations:\n",
    "- Crafting a straightforward set of rules to guide the lunar lander is more challenging than anticipated.\n",
    "- Our initial efforts achieved some stability, but eventually, the lander lost control.\n",
    "\n",
    "## Deep Reinforcement Learning\n",
    "To address this challenge, we'll use deep reinforcement learning techniques to train an agent to land the spacecraft.\n",
    "\n",
    "Simpler tabular methods are limited to discrete observation spaces, meaning there are a finite number of possible states. In `LunarLander-v2` howeer, we're dealing with a continuous range of states across 8 different parameters, meaning there are a near-infinite number of possible states. We could try to bin similar values into groups, but due to the sensitive controls of the game, even slight errors can lead to significant missteps.\n",
    "\n",
    "To get around this, we'll use a `neural network Q-function approximator`. This lets us predict the best actions to take for a given state, even when dealing with a vast number of potential states. It's a much better match for our complex landing challenge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
